# PhysIntuit: Physical Intuition Benchmark

A comprehensive benchmark for evaluating physical reasoning capabilities in vision-language models.

## Overview

PhysIntuit is designed to test and evaluate how well vision-language models can understand and reason about physical interactions in both 2D and 3D environments. Our benchmark includes a diverse set of physical scenarios, ranging from simple object interactions to complex chain reactions.

## Key Features

- **Comprehensive Coverage**: Tests various physical phenomena including gravity, force, and collision
- **Dual Format Evaluation**: Includes both binary classification and state prediction tasks
- **Multi-domain Testing**: Evaluates physical reasoning in both 2D and 3D environments
- **Balanced Dataset**: Carefully curated scenarios across different difficulty levels

## Installation

```bash
git clone https://github.com/kw1han/Physical_Intuition_test.git
cd Physical_Intuition_test
pip install -r requirements.txt
```

## Documentation

Visit our [project website](https://kw1han.github.io/Physical_Intuition_test/) for detailed documentation and examples.

## Citation

If you use PhysIntuit in your research, please cite our paper:

```bibtex
@article{physintuit2024,
  title={PhysIntuit: A Benchmark for Physical Intuition in Vision-Language Models},
  author={Your Name},
  journal={arXiv preprint},
  year={2024}
}
```

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details. 